{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction and Installation JAHS-Bench-201 is the first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search (JAHS), built to support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To install using pip run pip install jahs_bench Optionally, you can download the data required to use the surrogate benchmark ahead of time with python -m jahs_bench.download --target surrogates To test if the installation was successful, you can, e.g, run a minimal example with python -m jahs_bench_examples.minimal This should randomly sample a configuration, and display both the sampled configuration and the result of querying the surrogate for that configuration.","title":"Introduction and Installation"},{"location":"#introduction-and-installation","text":"JAHS-Bench-201 is the first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search (JAHS), built to support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To install using pip run pip install jahs_bench Optionally, you can download the data required to use the surrogate benchmark ahead of time with python -m jahs_bench.download --target surrogates To test if the installation was successful, you can, e.g, run a minimal example with python -m jahs_bench_examples.minimal This should randomly sample a configuration, and display both the sampled configuration and the result of querying the surrogate for that configuration.","title":"Introduction and Installation"},{"location":"authorstatement/","text":"The authors bear all responsibility in case of violation of rights.","title":"Author Statement"},{"location":"evaluation_protocol/","text":"For scientific studies we recommend an evaluation protocol when using JAHS-Bench-201 here to facilitate fair, reproducible, and methodologically sound comparisons. Optimization Tasks Studies with JAHS-Bench-201 should report on all three datasets and present results as trajectories of best validation error (single-objective) or hypervolume (multi-objective). Seeds and Errors Bounds Results should report the mean and standard error around the mean and feature a minimum of 10 seeds. Reporting Across Runtimes To allow using the same evaluation protocol and comparisons across various algorithmic settings the objective trajectories should be reported across total runtime taking into account the training and evaluation costs predicted by the surrogate benchmark. We suggest to report until a runtime corresponding to approximately 100 evaluations and, for interpretability, show the total runtime divided by the mean runtime of one evaluation. (Multi) Multi-fidelity For (multi) multi-fidelity runs utilizing epochs, we support both using continuations from few to many epochs (to simulate the checkpointing and continuation of the model) as well as retraining from scratch for optimizers that do not support continuations.","title":"Evaluation Protocol"},{"location":"evaluation_protocol/#optimization-tasks","text":"Studies with JAHS-Bench-201 should report on all three datasets and present results as trajectories of best validation error (single-objective) or hypervolume (multi-objective).","title":"Optimization Tasks"},{"location":"evaluation_protocol/#seeds-and-errors-bounds","text":"Results should report the mean and standard error around the mean and feature a minimum of 10 seeds.","title":"Seeds and Errors Bounds"},{"location":"evaluation_protocol/#reporting-across-runtimes","text":"To allow using the same evaluation protocol and comparisons across various algorithmic settings the objective trajectories should be reported across total runtime taking into account the training and evaluation costs predicted by the surrogate benchmark. We suggest to report until a runtime corresponding to approximately 100 evaluations and, for interpretability, show the total runtime divided by the mean runtime of one evaluation.","title":"Reporting Across Runtimes"},{"location":"evaluation_protocol/#multi-multi-fidelity","text":"For (multi) multi-fidelity runs utilizing epochs, we support both using continuations from few to many epochs (to simulate the checkpointing and continuation of the model) as well as retraining from scratch for optimizers that do not support continuations.","title":"(Multi) Multi-fidelity"},{"location":"leaderboards/","text":"JAHS-Bench-201 Leaderboards The leaderboards as explained in our paper. To add an entry please create a pull request changing docs/leaderboards.md . Make sure to follow our evaluation protocol and to add a URL to material that explains how to reproduce you results. Single Objective Black-box CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201 Cost-aware CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201 Multi-fidelity CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.78 \u00b1 0.14 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 95.27 \u00b1 0.04 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.13 \u00b1 0.03 Hyperband JAHS-Bench-201 Multi multi-fidelity CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.9 \u00b1 0.14 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.91 \u00b1 0.07 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.18 \u00b1 0.02 Hyperband JAHS-Bench-201 Multi Objective Black-box CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201 Cost-aware CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201 Multi-fidelity CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1080.26 \u00b1 1.36 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 347.52 \u00b1 1.96 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 277.52 \u00b1 1.07 Hyperband JAHS-Bench-201 Multi multi-fidelity CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 899.92 \u00b1 1.54 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 330.76 \u00b1 0.22 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 306.36 \u00b1 0.19 Hyperband JAHS-Bench-201","title":"Leaderboards"},{"location":"leaderboards/#jahs-bench-201-leaderboards","text":"The leaderboards as explained in our paper. To add an entry please create a pull request changing docs/leaderboards.md . Make sure to follow our evaluation protocol and to add a URL to material that explains how to reproduce you results.","title":"JAHS-Bench-201 Leaderboards"},{"location":"leaderboards/#single-objective","text":"","title":"Single Objective"},{"location":"leaderboards/#black-box","text":"","title":"Black-box"},{"location":"leaderboards/#cifar-10","text":"Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology","text":"Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist","text":"Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#cost-aware","text":"","title":"Cost-aware"},{"location":"leaderboards/#cifar-10_1","text":"Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_1","text":"Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_1","text":"Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-fidelity","text":"","title":"Multi-fidelity"},{"location":"leaderboards/#cifar-10_2","text":"Rank Accuracy \u00b1 SE Name URL 1 90.78 \u00b1 0.14 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_2","text":"Rank Accuracy \u00b1 SE Name URL 1 95.27 \u00b1 0.04 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_2","text":"Rank Accuracy \u00b1 SE Name URL 1 95.13 \u00b1 0.03 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-multi-fidelity","text":"","title":"Multi multi-fidelity"},{"location":"leaderboards/#cifar-10_3","text":"Rank Accuracy \u00b1 SE Name URL 1 90.9 \u00b1 0.14 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_3","text":"Rank Accuracy \u00b1 SE Name URL 1 94.91 \u00b1 0.07 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_3","text":"Rank Accuracy \u00b1 SE Name URL 1 95.18 \u00b1 0.02 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-objective","text":"","title":"Multi Objective"},{"location":"leaderboards/#black-box_1","text":"","title":"Black-box"},{"location":"leaderboards/#cifar-10_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#cost-aware_1","text":"","title":"Cost-aware"},{"location":"leaderboards/#cifar-10_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-fidelity_1","text":"","title":"Multi-fidelity"},{"location":"leaderboards/#cifar-10_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 1080.26 \u00b1 1.36 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 347.52 \u00b1 1.96 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 277.52 \u00b1 1.07 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-multi-fidelity_1","text":"","title":"Multi multi-fidelity"},{"location":"leaderboards/#cifar-10_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 899.92 \u00b1 1.54 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 330.76 \u00b1 0.22 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 306.36 \u00b1 0.19 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"performance_dataset/","text":"Details on Performance Dataset Downloading the Performance Dataset The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full raw metrics data for CIFAR-10, run wget ... Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users may then append the exact filename they wish to acces to the link. The following 4 files are available: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"\" # Full path to a downloaded file, ending in \".pkl.gz\" df = pd . read_pickle ( pth ) df . head ( 5 ) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Details on Performance Dataset"},{"location":"performance_dataset/#details-on-performance-dataset","text":"","title":"Details on Performance Dataset"},{"location":"performance_dataset/#downloading-the-performance-dataset","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full raw metrics data for CIFAR-10, run wget ...","title":"Downloading the Performance Dataset"},{"location":"performance_dataset/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users may then append the exact filename they wish to acces to the link. The following 4 files are available: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"\" # Full path to a downloaded file, ending in \".pkl.gz\" df = pd . read_pickle ( pth ) df . head ( 5 ) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Archive Structure"},{"location":"search_space/","text":"","title":"Search Space"},{"location":"surrogate/","text":"Details on Surrogate Models Downloading the Surrogate Models The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as g-zipped tarballs that are readable using our code base. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full set of surrogates for CIFAR-10, run wget ... Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users can either recursively download the entire directory tree rooted at that sub-directory and pass it to our top-level API in order to load all the models for a particular task or downloaded individual sub-directories and use the more granular API to load surrogates for individual metrics. Each sub-sub-directory is named after the particular metric the model contained within was trained to predict and contains a number of tarballs that contain the relevant data needed to load a trained model into memory. The downloaded models can be individually loaded into memory as: from jahs_bench.surrogate.model import XGBSurrogate pth = \"\" # Full path to a model directory (e.g. \"../cifar10/latency\") model = XGBSurrogate . load () The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Details on Surrogate Models"},{"location":"surrogate/#details-on-surrogate-models","text":"","title":"Details on Surrogate Models"},{"location":"surrogate/#downloading-the-surrogate-models","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as g-zipped tarballs that are readable using our code base. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full set of surrogates for CIFAR-10, run wget ...","title":"Downloading the Surrogate Models"},{"location":"surrogate/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users can either recursively download the entire directory tree rooted at that sub-directory and pass it to our top-level API in order to load all the models for a particular task or downloaded individual sub-directories and use the more granular API to load surrogates for individual metrics. Each sub-sub-directory is named after the particular metric the model contained within was trained to predict and contains a number of tarballs that contain the relevant data needed to load a trained model into memory. The downloaded models can be individually loaded into memory as: from jahs_bench.surrogate.model import XGBSurrogate pth = \"\" # Full path to a model directory (e.g. \"../cifar10/latency\") model = XGBSurrogate . load () The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Archive Structure"},{"location":"todo/","text":"TODO Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" * Dataset Documentation * URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) * Instructions on accessing the dataset Danny * Go over experiments repo * Go over README + documentation * Put on pypi","title":"Todo"},{"location":"todo/#todo","text":"Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" * Dataset Documentation * URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) * Instructions on accessing the dataset Danny * Go over experiments repo * Go over README + documentation * Put on pypi","title":"TODO"},{"location":"usage/","text":"Querying JAHS-Bench-201 Evaluating Configurations with The Surrogate import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , download = True ) # Query a random configuration config = benchmark . sample_config () results = benchmark ( config ) # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict Querying the Full Trajectories Optionally, the full trajectory of query can be queried by flipping a single flag config , trajectory = benchmark . random_sample ( full_trajectory = True ) print ( trajectory ) # A list of dicts More Evaluation Options The API of our benchmark enables users to either query a surrogate model (the default) or the tables of performance data, or train a configuration from our search space from scratch using the same pipeline as was used by our benchmark. However, users should note that the latter functionality requires the installation of jahs_bench_201 with the optional data_creation component and its relevant dependencies. The relevant data can be automatically downloaded by our API. Querying the Surrogate # Download the trained surrogate model import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"surrogate\" , download = True ) # Query a random configuration config , results = benchmark . random_sample () # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict Querying the Performance Tables # Download the performance dataset import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) # Query a random configuration config , results = benchmark . random_sample () # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict Live Training a Random Configuration from Scratch # Initialize the pipeline import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"live\" ) # Query a random configuration config , results = benchmark . random_sample () # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # Only the final epochs' results","title":"Querying the Benchmark"},{"location":"usage/#querying-jahs-bench-201","text":"","title":"Querying JAHS-Bench-201"},{"location":"usage/#evaluating-configurations-with-the-surrogate","text":"import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , download = True ) # Query a random configuration config = benchmark . sample_config () results = benchmark ( config ) # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict","title":"Evaluating Configurations with The Surrogate"},{"location":"usage/#querying-the-full-trajectories","text":"Optionally, the full trajectory of query can be queried by flipping a single flag config , trajectory = benchmark . random_sample ( full_trajectory = True ) print ( trajectory ) # A list of dicts","title":"Querying the Full Trajectories"},{"location":"usage/#more-evaluation-options","text":"The API of our benchmark enables users to either query a surrogate model (the default) or the tables of performance data, or train a configuration from our search space from scratch using the same pipeline as was used by our benchmark. However, users should note that the latter functionality requires the installation of jahs_bench_201 with the optional data_creation component and its relevant dependencies. The relevant data can be automatically downloaded by our API.","title":"More Evaluation Options"},{"location":"usage/#querying-the-surrogate","text":"# Download the trained surrogate model import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"surrogate\" , download = True ) # Query a random configuration config , results = benchmark . random_sample () # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict","title":"Querying the Surrogate"},{"location":"usage/#querying-the-performance-tables","text":"# Download the performance dataset import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) # Query a random configuration config , results = benchmark . random_sample () # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict","title":"Querying the Performance Tables"},{"location":"usage/#live-training-a-random-configuration-from-scratch","text":"# Initialize the pipeline import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"live\" ) # Query a random configuration config , results = benchmark . random_sample () # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # Only the final epochs' results","title":"Live Training a Random Configuration from Scratch"}]}