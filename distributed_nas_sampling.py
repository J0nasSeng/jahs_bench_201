import argparse
import codecs
import json
import logging
import os
import sys
import time
from pathlib import Path

import torch
import numpy as np

from naslib.search_spaces import NASB201HPOSearchSpace
import naslib.utils.utils as naslib_utils
import naslib.utils.logging as naslib_logging

init_time = time.time()

parser = argparse.ArgumentParser()
parser.add_argument("--basedir", type=Path, default=Path().cwd(),
                    help="Path to the base directory where all the tasks' output will be stored. Task-specific "
                         "sub-directories will be created here if needed.")
parser.add_argument("--taskid", type=int,
                    help="An offset from 0 for this task within the current node's allocation of tasks.")
parser.add_argument("--epochs", type=int, default=25,
                    help="Number of epochs that each sampled architecture should be trained for. Default: 25")
parser.add_argument("--resize", type=int, default=8,
                    help="An integer value (8, 16, 32, ...) to determine the scaling of input images. Default: 8")
parser.add_argument("--global-seed", type=int, default=None,
                    help="A value for a global seed to be used for all global NumPy and PyTorch random operations. "
                         "This is different from the fixed seed used for reproducible search space sampling. If not "
                         "specified, a random source of entropy is used instead.")
parser.add_argument("--standalone-mode", action="store_true",
                    help="Switch that enables working in a single-task local setup as opposed to the default "
                         "multi-node cluster setup.")
parser.add_argument("--debug", action="store_true", help="Enable debug mode (very verbose) logging.")
args = parser.parse_args()

basedir = args.basedir
taskid = args.taskid
epochs = args.epochs
resize = args.resize
global_seed = args.global_seed

taskdir: Path = basedir / str(taskid)
outdir: Path = taskdir / "benchmark_data"
outdir.mkdir(exist_ok=True, parents=True)

# Randomly generated entropy source, to remain fixed across experiments.
seed = 79029434164686768057103648623012072794
# Pseudo-RNG should rely on a bit-stream that is largely uncorrelated both within and across tasks
rng = np.random.RandomState(np.random.Philox(seed=seed, counter=taskid))

if global_seed is None:
    global_seed = int(np.random.default_rng().integers(0, 2**32 - 1))

# Read args and config, setup logger
naslib_args = naslib_utils.default_argument_parser().parse_args([
        # "config_file": "%s/defaults/nas_sampling.yaml" % (naslib_utils.get_project_root()),
        f"--seed={str(global_seed)}",
        f"--resize={str(resize)}",
        "out_dir", str(taskdir),
        "search.epochs", str(epochs)
    ])

naslib_config = naslib_utils.get_config_from_args(naslib_args, config_type="nas")
naslib_utils.set_seed(naslib_config.seed)

logger = naslib_logging.setup_logger(naslib_config.save + "/log.log")
if args.debug:
    logger.setLevel(logging.DEBUG)
else:
    logger.setLevel(logging.WARNING)
# logger.setLevel(logging.INFO)   # default DEBUG is very verbose
naslib_utils.log_args(naslib_config)

def init_adam(model):
    config = model.config
    lr, weight_decay = config["learning_rate"], config["weight_decay"]
    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    return optim

def init_adamw(model):
    config = model.config
    lr, weight_decay = config["learning_rate"], config["weight_decay"]
    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    return optim

def init_sgd(model):
    config = model.config
    lr, momentum, weight_decay = config["learning_rate"], config["momentum"], config["weight_decay"]
    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)
    return optim

optimizer_constructor = {
    "Adam": init_adam,
    "AdamW": init_adamw,
    "SGD": init_sgd,
}


def train(model, data_loaders, train_config):
    """
    Train the given model using the given data loaders and training configuration. Returns a dict containing various metrics.

    Parameters
    ----------
    model: naslib.search_spaces.Graph
        A NASLib object obtained by sampling a random architecture on a search space. Ideally, the sampled object should
        be cloned before being passsed. The model will be parsed to PyTorch before training.
    data_loaders: Tuple
        A tuple of objects that correspond to the data loaders generated by naslib.utils.utils.get_train_val_loaders().
    train_config: naslib.utils.utils.AttrDict
        An attribute dict containing various configuration parameters for the model training.
    """
    start_time = time.time()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    logger.debug(f"Training model with config: {model.config}")
    model.parse()
    model = model.to(device)

    errors_dict, metrics = get_metrics(model)
    train_queue, valid_queue, test_queue, _, _ = data_loaders

    optim = optimizer_constructor[model.config["optimizer"]](model)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=train_config.epochs, eta_min=0.)
    loss = torch.nn.CrossEntropyLoss()

    train_start_time = time.time()
    for e in range(train_config.epochs):
        for step, ((train_inputs, train_labels), (val_inputs, val_labels)) in \
                enumerate(zip(train_queue, valid_queue)):
            train_inputs = train_inputs.to(device)
            train_labels = train_labels.to(device)
            optim.zero_grad()
            logits_train = model(train_inputs)
            train_loss = loss(logits_train, train_labels)
            train_loss.backward()
            optim.step()

            val_inputs = val_inputs.to(device)
            val_labels = val_labels.to(device)
            logits_val = model(val_inputs)
            val_loss = loss(logits_val, val_labels)

            naslib_logging.log_every_n_seconds(
                logging.DEBUG,
                "Epoch {}-{}, Train loss: {:.5f}, validation loss: {:.5f}".format(e, step, train_loss, val_loss),
                n=15, name=logger.name
            )

            metrics.train_loss.update(float(train_loss.detach().cpu()))
            metrics.val_loss.update(float(val_loss.detach().cpu()))
            update_accuracies(metrics, logits_train, train_labels, "train")
            update_accuracies(metrics, logits_val, val_labels, "val")

        scheduler.step()

        errors_dict.train_acc.append(metrics.train_acc.avg)
        errors_dict.train_loss.append(metrics.train_loss.avg)
        errors_dict.val_acc.append(metrics.val_acc.avg)
        errors_dict.val_loss.append(metrics.val_loss.avg)

    train_end_time = time.time()

    for (test_inputs, test_labels) in test_queue:
        test_inputs = test_inputs.to(device)
        test_labels = test_labels.to(device)
        logits_test = model(test_inputs)
        test_loss = loss(logits_test, test_labels)
        metrics.test_loss.update(float(test_loss.detach().cpu()))
        update_accuracies(metrics, logits_test, test_labels, "test")

    end_time = time.time()

    errors_dict.test_acc = metrics.test_acc.avg
    errors_dict.test_loss = metrics.test_loss.avg
    errors_dict.runtime = end_time - start_time
    errors_dict.train_time = train_end_time - train_start_time

    metrics.train_acc.reset()
    metrics.train_loss.reset()
    metrics.val_acc.reset()
    metrics.val_loss.reset()

    return errors_dict


def get_metrics(model):
    errors_dict = naslib_utils.AttrDict(
        {'train_acc': [],
         'train_loss': [],
         'val_acc': [],
         'val_loss': [],
         'test_acc': None,
         'test_loss': None,
         'runtime': None,
         'train_time': None,
         'model_size_MB': naslib_utils.count_parameters_in_MB(model)}
    )

    metrics = naslib_utils.AttrDict({
        'train_acc': naslib_utils.AverageMeter(),
        'train_loss': naslib_utils.AverageMeter(),
        'val_acc': naslib_utils.AverageMeter(),
        'val_loss': naslib_utils.AverageMeter(),
        'test_acc': naslib_utils.AverageMeter(),
        'test_loss': naslib_utils.AverageMeter(),
    })

    return errors_dict, metrics


def update_accuracies(metrics, logits, target, split):
    """Update the accuracy counters"""
    logits = logits.clone().detach().cpu()
    target = target.clone().detach().cpu()
    acc, _ = naslib_utils.accuracy(logits, target, topk=(1, 5))
    n = logits.size(0)

    if split == 'train':
        metrics.train_acc.update(acc.data.item(), n)
    elif split == 'val':
        metrics.val_acc.update(acc.data.item(), n)
    elif split == 'test':
        metrics.test_acc.update(acc.data.item(), n)
    else:
        raise ValueError("Unknown split: {}. Expected either 'train' or 'val'")


search_space = NASB201HPOSearchSpace()

data_loaders_start_wctime = time.time()
data_loaders_start_ptime = time.process_time()

data_loaders = naslib_utils.get_train_val_loaders(naslib_config, mode='train')

data_loaders_end_wctime = time.time()
data_loaders_end_ptime = time.process_time()

init_duration = data_loaders_start_wctime - init_time
data_loaders_wc_duration = data_loaders_end_wctime - data_loaders_start_wctime
data_loaders_proc_duration = data_loaders_end_ptime - data_loaders_start_ptime

with open(outdir / "meta.json", "w") as fp:
    json.dump(dict(
        init_duration=init_duration,
        wc_duration=data_loaders_wc_duration,
        proc_duration=data_loaders_proc_duration,
        resize=resize,
        epochs=epochs
    ), fp, indent=4)

n_archs = 0
while True:
    naslib_logging.log_every_n_seconds(logging.DEBUG, f"Sampling architecture #{n_archs}.", 15, name=logger.name)
    model: NASB201HPOSearchSpace = search_space.clone()
    model.sample_random_architecture(rng=rng)
    try:
        res = train(model=model, data_loaders=data_loaders, train_config=naslib_config.search)
    except Exception as e:
        res = {"exception": str(e)}
    res["config"] = model.config.get_dictionary()
    naslib_logging.log_every_n_seconds(logging.DEBUG, "Finished training architecture.", 15, name=logger.name)
    n_archs += 1
    with open(outdir / f"{n_archs}.json", "w") as fp:
        json.dump(res, fp, indent=4)

