{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction JAHS-Bench-201 is the first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search, built to support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. Installation Using pip pip install jahs_bench_201 Verify Installation This is a minimum working example to test if the installation was successful: python -m jahs_bench_201_examples.mwe This should randomly sample a configuration and display both the sampled configuration and the result of querying the surrogate for that configuration. Usage","title":"Introduction"},{"location":"#introduction","text":"JAHS-Bench-201 is the first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search, built to support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms.","title":"Introduction"},{"location":"#installation","text":"Using pip pip install jahs_bench_201","title":"Installation"},{"location":"#verify-installation","text":"This is a minimum working example to test if the installation was successful: python -m jahs_bench_201_examples.mwe This should randomly sample a configuration and display both the sampled configuration and the result of querying the surrogate for that configuration.","title":"Verify Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"download_dataset/","text":"Downloading the Performance Dataset The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full raw metrics data for CIFAR-10, run wget ... Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users may then append the exact filename they wish to acces to the link. The following 4 files are available: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"\" # Full path to a downloaded file, ending in \".pkl.gz\" df = pd.read_pickle(pth) df.head(5) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Downloading the Performance Dataset"},{"location":"download_dataset/#downloading-the-performance-dataset","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full raw metrics data for CIFAR-10, run wget ...","title":"Downloading the Performance Dataset"},{"location":"download_dataset/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users may then append the exact filename they wish to acces to the link. The following 4 files are available: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"\" # Full path to a downloaded file, ending in \".pkl.gz\" df = pd.read_pickle(pth) df.head(5) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Archive Structure"},{"location":"download_surrogate/","text":"Downloading the Surrogate Models The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as g-zipped tarballs that are readable using our code base. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full set of surrogates for CIFAR-10, run wget ... Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users can either recursively download the entire directory tree rooted at that sub-directory and pass it to our top-level API in order to load all the models for a particular task or downloaded individual sub-directories and use the more granular API to load surrogates for individual metrics. Each sub-sub-directory is named after the particular metric the model contained within was trained to predict and contains a number of tarballs that contain the relevant data needed to load a trained model into memory. The downloaded models can be individually loaded into memory as: from jahs_bench_201.surrogate.model import XGBSurrogate pth = \"\" # Full path to a model directory (e.g. \"../cifar10/latency\") model = XGBSurrogate.load() The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Downloading the Surrogate Models"},{"location":"download_surrogate/#downloading-the-surrogate-models","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as g-zipped tarballs that are readable using our code base. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full set of surrogates for CIFAR-10, run wget ...","title":"Downloading the Surrogate Models"},{"location":"download_surrogate/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users can either recursively download the entire directory tree rooted at that sub-directory and pass it to our top-level API in order to load all the models for a particular task or downloaded individual sub-directories and use the more granular API to load surrogates for individual metrics. Each sub-sub-directory is named after the particular metric the model contained within was trained to predict and contains a number of tarballs that contain the relevant data needed to load a trained model into memory. The downloaded models can be individually loaded into memory as: from jahs_bench_201.surrogate.model import XGBSurrogate pth = \"\" # Full path to a model directory (e.g. \"../cifar10/latency\") model = XGBSurrogate.load() The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Archive Structure"},{"location":"evaluation_protocol/","text":"We accompany JAHS-Bench-201 with evaluation protocols and code to aid following these protocols, to facilitate fair, reproducible, and methodologically sound comparisons. Studies with JAHS-Bench-201 should report on all three datasets and present results as trajectories of best validation error (single-objective) or hypervolume (multi-objective). To allow using the same evaluation protocol and comparisons across the various algorithmic settings above, these trajectories should be reported across total runtime taking into account the training and evaluation costs predicted by the surrogate benchmark. We suggest to report until a runtime corresponding to approximately 100 evaluations and, for interpretability, show the total runtime divided by the mean runtime of one evaluation. Further, in (multi) multi-fidelity runs utilizing epochs, we support both using continuations from few to many epochs (to simulate the checkpointing and continuation of the model) as well as retraining from scratch for optimizers that do not support continuations. Results should report the mean and standard error around the mean and feature a minimum of 10 seeds.","title":"Evaluation Protocol"},{"location":"performance_dataset/","text":"","title":"Analysis of Performance Dataset"},{"location":"surrogate/","text":"","title":"Creation of Surrogate Benchmarks"},{"location":"todo/","text":"TODO Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" Danny * Placeholders Documentation * Go over README * Experiments repo fill Maciej * We provide code to follow our evaluation protocols Open * Put on pypi * Fix: The 'automl/jahs_bench_201' repository doesn't contain the 'TODO' path in 'main'. From appendix copied: Dataset Documentation \\todo{Mostly requirements from NeurIPS} URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) Instructions on accessing the dataset Datasheets - for us, this would be broad properties such as the format, disk space requirements, table dimensions, other metadata. \\todo{Is there a standard framework that we can use?} License and author statement, ethical/responsible use guidelines. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license. Recommended from the guidelines: accountability framework Hosting, licensing and maintenance plan. Clarify long-term preservation. Highly recommended: a persistent dereferenceable identifier, e.g. DOI or prefix from identifiers.org. (We already are on GitHub) Compute usage Ethics statement API/Git Repo [Referenced by section 1] Detailed instructions for using the dataset. Minimum Working Example(s) Reproducibility documentation - instructions, data, code.","title":"Todo"},{"location":"todo/#todo","text":"Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" Danny * Placeholders Documentation * Go over README * Experiments repo fill Maciej * We provide code to follow our evaluation protocols Open * Put on pypi * Fix: The 'automl/jahs_bench_201' repository doesn't contain the 'TODO' path in 'main'. From appendix copied: Dataset Documentation \\todo{Mostly requirements from NeurIPS} URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) Instructions on accessing the dataset Datasheets - for us, this would be broad properties such as the format, disk space requirements, table dimensions, other metadata. \\todo{Is there a standard framework that we can use?} License and author statement, ethical/responsible use guidelines. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license. Recommended from the guidelines: accountability framework Hosting, licensing and maintenance plan. Clarify long-term preservation. Highly recommended: a persistent dereferenceable identifier, e.g. DOI or prefix from identifiers.org. (We already are on GitHub) Compute usage Ethics statement API/Git Repo [Referenced by section 1] Detailed instructions for using the dataset. Minimum Working Example(s) Reproducibility documentation - instructions, data, code.","title":"TODO"},{"location":"usage/","text":"","title":"Using the Benchmark"}]}