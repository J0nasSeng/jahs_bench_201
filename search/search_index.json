{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction TODO","title":"Introduction"},{"location":"#introduction","text":"TODO","title":"Introduction"},{"location":"download_dataset/","text":"Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Additionally, we are aware of the inherent isssues with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather.","title":"Downloading the Performance Dataset"},{"location":"download_surrogate/","text":"","title":"Downloading the Surrogate Models"},{"location":"performance_dataset/","text":"","title":"Analysis of Performance Dataset"},{"location":"surrogate/","text":"","title":"Creation of Surrogate Benchmarks"},{"location":"todo/","text":"TODO Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" Danny * python -m jahs_bench_201.download_surrogates and download=True as default (danny) * write leaderboards text * Placeholders Documentation * https://github.com/automl/jahs_bench_201_experiments mention * Go over README * Experiments repo fill Maciej * We provide code to use JAHS-Bench-201 and follow our evaluation protocols (latter Maciej) * leaderboard entry (Maciej) Open * Documentation at https://automl.github.io/jahs_bench_201/ * Put on pypi * Fix: The 'automl/jahs_bench_201' repository doesn't contain the 'TODO' path in 'main'. From appendix copied: * Dataset Documentation \\todo{Mostly requirements from NeurIPS} * URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) * Instructions on accessing the dataset * Datasheets - for us, this would be broad properties such as the format, disk space requirements, table dimensions, other metadata. \\todo{Is there a standard framework that we can use?} * License and author statement, ethical/responsible use guidelines. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license. * Recommended from the guidelines: accountability framework * Hosting, licensing and maintenance plan. Clarify long-term preservation. * Highly recommended: a persistent dereferenceable identifier, e.g. DOI or prefix from identifiers.org. (We already are on GitHub) * Compute usage * Ethics statement * API/Git Repo [Referenced by section 1] * Detailed instructions for using the dataset. * Minimum Working Example(s) * Reproducibility documentation - instructions, data, code.","title":"Todo"},{"location":"todo/#todo","text":"Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" Danny * python -m jahs_bench_201.download_surrogates and download=True as default (danny) * write leaderboards text * Placeholders Documentation * https://github.com/automl/jahs_bench_201_experiments mention * Go over README * Experiments repo fill Maciej * We provide code to use JAHS-Bench-201 and follow our evaluation protocols (latter Maciej) * leaderboard entry (Maciej) Open * Documentation at https://automl.github.io/jahs_bench_201/ * Put on pypi * Fix: The 'automl/jahs_bench_201' repository doesn't contain the 'TODO' path in 'main'. From appendix copied: * Dataset Documentation \\todo{Mostly requirements from NeurIPS} * URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) * Instructions on accessing the dataset * Datasheets - for us, this would be broad properties such as the format, disk space requirements, table dimensions, other metadata. \\todo{Is there a standard framework that we can use?} * License and author statement, ethical/responsible use guidelines. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license. * Recommended from the guidelines: accountability framework * Hosting, licensing and maintenance plan. Clarify long-term preservation. * Highly recommended: a persistent dereferenceable identifier, e.g. DOI or prefix from identifiers.org. (We already are on GitHub) * Compute usage * Ethics statement * API/Git Repo [Referenced by section 1] * Detailed instructions for using the dataset. * Minimum Working Example(s) * Reproducibility documentation - instructions, data, code.","title":"TODO"}]}