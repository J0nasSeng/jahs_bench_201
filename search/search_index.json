{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction and Installation The first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search, built to support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To install using pip run pip install jahs_bench Optionally, you can download the data required to use the surrogate benchmark ahead of time with python -m jahs_bench.download --target surrogates To test if the installation was successful, you can, e.g, run a minimal example with python -m jahs_bench_examples.minimal This should randomly sample a configuration, and display both the sampled configuration and the result of querying the surrogate for that configuration.","title":"Introduction and Installation"},{"location":"#introduction-and-installation","text":"The first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search, built to support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To install using pip run pip install jahs_bench Optionally, you can download the data required to use the surrogate benchmark ahead of time with python -m jahs_bench.download --target surrogates To test if the installation was successful, you can, e.g, run a minimal example with python -m jahs_bench_examples.minimal This should randomly sample a configuration, and display both the sampled configuration and the result of querying the surrogate for that configuration.","title":"Introduction and Installation"},{"location":"evaluation_protocol/","text":"We accompany JAHS-Bench-201 with evaluation protocols and code to aid following these protocols, to facilitate fair, reproducible, and methodologically sound comparisons. Studies with JAHS-Bench-201 should report on all three datasets and present results as trajectories of best validation error (single-objective) or hypervolume (multi-objective). To allow using the same evaluation protocol and comparisons across the various algorithmic settings above, these trajectories should be reported across total runtime taking into account the training and evaluation costs predicted by the surrogate benchmark. We suggest to report until a runtime corresponding to approximately 100 evaluations and, for interpretability, show the total runtime divided by the mean runtime of one evaluation. Further, in (multi) multi-fidelity runs utilizing epochs, we support both using continuations from few to many epochs (to simulate the checkpointing and continuation of the model) as well as retraining from scratch for optimizers that do not support continuations. Results should report the mean and standard error around the mean and feature a minimum of 10 seeds.","title":"Evaluation Protocol"},{"location":"leaderboards/","text":"JAHS-Bench-201 Leaderboards The leaderboards as explained in our paper. To add an entry please create a pull request changing docs/leaderboards.md . Make sure to follow our evaluation protocol and to add a URL to material that explains how to reproduce you results. Single Objective Black-box CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201 Cost-aware CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201 Multi-fidelity CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.78 \u00b1 0.14 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 95.27 \u00b1 0.04 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.13 \u00b1 0.03 Hyperband JAHS-Bench-201 Multi multi-fidelity CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.9 \u00b1 0.14 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.91 \u00b1 0.07 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.18 \u00b1 0.02 Hyperband JAHS-Bench-201 Multi Objective Black-box CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201 Cost-aware CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201 Multi-fidelity CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1080.26 \u00b1 1.36 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 347.52 \u00b1 1.96 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 277.52 \u00b1 1.07 Hyperband JAHS-Bench-201 Multi multi-fidelity CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 899.92 \u00b1 1.54 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 330.76 \u00b1 0.22 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 306.36 \u00b1 0.19 Hyperband JAHS-Bench-201","title":"Leaderboards"},{"location":"leaderboards/#jahs-bench-201-leaderboards","text":"The leaderboards as explained in our paper. To add an entry please create a pull request changing docs/leaderboards.md . Make sure to follow our evaluation protocol and to add a URL to material that explains how to reproduce you results.","title":"JAHS-Bench-201 Leaderboards"},{"location":"leaderboards/#single-objective","text":"","title":"Single Objective"},{"location":"leaderboards/#black-box","text":"","title":"Black-box"},{"location":"leaderboards/#cifar-10","text":"Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology","text":"Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist","text":"Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#cost-aware","text":"","title":"Cost-aware"},{"location":"leaderboards/#cifar-10_1","text":"Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_1","text":"Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_1","text":"Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-fidelity","text":"","title":"Multi-fidelity"},{"location":"leaderboards/#cifar-10_2","text":"Rank Accuracy \u00b1 SE Name URL 1 90.78 \u00b1 0.14 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_2","text":"Rank Accuracy \u00b1 SE Name URL 1 95.27 \u00b1 0.04 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_2","text":"Rank Accuracy \u00b1 SE Name URL 1 95.13 \u00b1 0.03 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-multi-fidelity","text":"","title":"Multi multi-fidelity"},{"location":"leaderboards/#cifar-10_3","text":"Rank Accuracy \u00b1 SE Name URL 1 90.9 \u00b1 0.14 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_3","text":"Rank Accuracy \u00b1 SE Name URL 1 94.91 \u00b1 0.07 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_3","text":"Rank Accuracy \u00b1 SE Name URL 1 95.18 \u00b1 0.02 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-objective","text":"","title":"Multi Objective"},{"location":"leaderboards/#black-box_1","text":"","title":"Black-box"},{"location":"leaderboards/#cifar-10_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#cost-aware_1","text":"","title":"Cost-aware"},{"location":"leaderboards/#cifar-10_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-fidelity_1","text":"","title":"Multi-fidelity"},{"location":"leaderboards/#cifar-10_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 1080.26 \u00b1 1.36 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 347.52 \u00b1 1.96 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 277.52 \u00b1 1.07 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-multi-fidelity_1","text":"","title":"Multi multi-fidelity"},{"location":"leaderboards/#cifar-10_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 899.92 \u00b1 1.54 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 330.76 \u00b1 0.22 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 306.36 \u00b1 0.19 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"performance_dataset/","text":"Details on Performance Dataset Downloading the Performance Dataset The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full raw metrics data for CIFAR-10, run wget ... Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users may then append the exact filename they wish to acces to the link. The following 4 files are available: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"\" # Full path to a downloaded file, ending in \".pkl.gz\" df = pd.read_pickle(pth) df.head(5) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Details on Performance Dataset"},{"location":"performance_dataset/#details-on-performance-dataset","text":"","title":"Details on Performance Dataset"},{"location":"performance_dataset/#downloading-the-performance-dataset","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full raw metrics data for CIFAR-10, run wget ...","title":"Downloading the Performance Dataset"},{"location":"performance_dataset/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users may then append the exact filename they wish to acces to the link. The following 4 files are available: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"\" # Full path to a downloaded file, ending in \".pkl.gz\" df = pd.read_pickle(pth) df.head(5) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Archive Structure"},{"location":"surrogate/","text":"Details on Surrogate Models Downloading the Surrogate Models The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as g-zipped tarballs that are readable using our code base. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full set of surrogates for CIFAR-10, run wget ... Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users can either recursively download the entire directory tree rooted at that sub-directory and pass it to our top-level API in order to load all the models for a particular task or downloaded individual sub-directories and use the more granular API to load surrogates for individual metrics. Each sub-sub-directory is named after the particular metric the model contained within was trained to predict and contains a number of tarballs that contain the relevant data needed to load a trained model into memory. The downloaded models can be individually loaded into memory as: from jahs_bench.surrogate.model import XGBSurrogate pth = \"\" # Full path to a model directory (e.g. \"../cifar10/latency\") model = XGBSurrogate.load() The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Details on Surrogate Models"},{"location":"surrogate/#details-on-surrogate-models","text":"","title":"Details on Surrogate Models"},{"location":"surrogate/#downloading-the-surrogate-models","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as g-zipped tarballs that are readable using our code base. The most convenient method for downloading our datasets is through our API. Nevertheless, interested users may directly download our DataFrames using file transfer software, such as wget , from our archive here For example, to download the full set of surrogates for CIFAR-10, run wget ...","title":"Downloading the Surrogate Models"},{"location":"surrogate/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task should be appended to the above archive link, as \"...metric_data/cifar10\", in order to access the sub-directory of that dataset. Following this, users can either recursively download the entire directory tree rooted at that sub-directory and pass it to our top-level API in order to load all the models for a particular task or downloaded individual sub-directories and use the more granular API to load surrogates for individual metrics. Each sub-sub-directory is named after the particular metric the model contained within was trained to predict and contains a number of tarballs that contain the relevant data needed to load a trained model into memory. The downloaded models can be individually loaded into memory as: from jahs_bench.surrogate.model import XGBSurrogate pth = \"\" # Full path to a model directory (e.g. \"../cifar10/latency\") model = XGBSurrogate.load() The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Archive Structure"},{"location":"todo/","text":"TODO Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" Danny * Placeholders Documentation * Go over README * Experiments repo fill Open * Put on pypi * Fix: The 'automl/jahs_bench_201' repository doesn't contain the 'TODO' path in 'main'. From appendix copied: Dataset Documentation \\todo{Mostly requirements from NeurIPS} URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) Instructions on accessing the dataset Datasheets - for us, this would be broad properties such as the format, disk space requirements, table dimensions, other metadata. \\todo{Is there a standard framework that we can use?} License and author statement, ethical/responsible use guidelines. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license. Recommended from the guidelines: accountability framework Hosting, licensing and maintenance plan. Clarify long-term preservation. Highly recommended: a persistent dereferenceable identifier, e.g. DOI or prefix from identifiers.org. (We already are on GitHub) Compute usage Ethics statement API/Git Repo [Referenced by section 1] Detailed instructions for using the dataset. Minimum Working Example(s) Reproducibility documentation - instructions, data, code.","title":"Todo"},{"location":"todo/#todo","text":"Archit: * pip install neural-pipeline-search for HPO of surrogate (Archit) -- include in documentation * Reference section of Readme/Documentation explaining how to install optional components in the section \"Usage\" Danny * Placeholders Documentation * Go over README * Experiments repo fill Open * Put on pypi * Fix: The 'automl/jahs_bench_201' repository doesn't contain the 'TODO' path in 'main'. From appendix copied: Dataset Documentation \\todo{Mostly requirements from NeurIPS} URL to the dataset and its metadata (must be structured; the guidelines mention using a web standard like schema.org or DCAT for this) Instructions on accessing the dataset Datasheets - for us, this would be broad properties such as the format, disk space requirements, table dimensions, other metadata. \\todo{Is there a standard framework that we can use?} License and author statement, ethical/responsible use guidelines. Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license. Recommended from the guidelines: accountability framework Hosting, licensing and maintenance plan. Clarify long-term preservation. Highly recommended: a persistent dereferenceable identifier, e.g. DOI or prefix from identifiers.org. (We already are on GitHub) Compute usage Ethics statement API/Git Repo [Referenced by section 1] Detailed instructions for using the dataset. Minimum Working Example(s) Reproducibility documentation - instructions, data, code.","title":"TODO"},{"location":"usage/","text":"Using JAHS-Bench-201 Querying the surrogate # Download the trained surrogate model import jahs_bench benchmark = jahs_bench.Benchmark(task=\"cifar10\", kind=\"surrogate\", download=True) # Query a random configuration config, results = benchmark.random_sample() # Display the outputs print(f\"Config: {config}\") # A dict print(f\"Result: {results}\") # A dict Querying the performance tables # Download the performance dataset import jahs_bench benchmark = jahs_bench.Benchmark(task=\"cifar10\", kind=\"table\", download=True) # Query a random configuration config, results = benchmark.random_sample() # Display the outputs print(f\"Config: {config}\") # A dict print(f\"Result: {results}\") # A dict Live training a random configuration from scratch # Initialize the pipeline import jahs_bench benchmark = jahs_bench.Benchmark(task=\"cifar10\", kind=\"live\") # Query a random configuration config, results = benchmark.random_sample() # Display the outputs print(f\"Config: {config}\") # A dict print(f\"Result: {results}\") # Only the final epochs' results Querying the full trajectories Optionally, the full trajectory of query can be queried by flipping a single flag config, trajectory = benchmark.random_sample(full_trajectory=True) print(trajectory) # A list of dicts","title":"Using the Benchmark"},{"location":"usage/#using-jahs-bench-201","text":"","title":"Using JAHS-Bench-201"},{"location":"usage/#querying-the-surrogate","text":"# Download the trained surrogate model import jahs_bench benchmark = jahs_bench.Benchmark(task=\"cifar10\", kind=\"surrogate\", download=True) # Query a random configuration config, results = benchmark.random_sample() # Display the outputs print(f\"Config: {config}\") # A dict print(f\"Result: {results}\") # A dict","title":"Querying the surrogate"},{"location":"usage/#querying-the-performance-tables","text":"# Download the performance dataset import jahs_bench benchmark = jahs_bench.Benchmark(task=\"cifar10\", kind=\"table\", download=True) # Query a random configuration config, results = benchmark.random_sample() # Display the outputs print(f\"Config: {config}\") # A dict print(f\"Result: {results}\") # A dict","title":"Querying the performance tables"},{"location":"usage/#live-training-a-random-configuration-from-scratch","text":"# Initialize the pipeline import jahs_bench benchmark = jahs_bench.Benchmark(task=\"cifar10\", kind=\"live\") # Query a random configuration config, results = benchmark.random_sample() # Display the outputs print(f\"Config: {config}\") # A dict print(f\"Result: {results}\") # Only the final epochs' results","title":"Live training a random configuration from scratch"},{"location":"usage/#querying-the-full-trajectories","text":"Optionally, the full trajectory of query can be queried by flipping a single flag config, trajectory = benchmark.random_sample(full_trajectory=True) print(trajectory) # A list of dicts","title":"Querying the full trajectories"}]}