{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction and Installation JAHS-Bench-201 is the first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search (JAHS), built to also support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To install using pip run pip install git+https://github.com/automl/jahs_bench_201.git Optionally, you can download the data required to use the surrogate benchmark ahead of time with python -m jahs_bench.download --target surrogates To test if the installation was successful, you can, e.g, run a minimal example with python -m jahs_bench_examples.minimal This should randomly sample a configuration, and display both the sampled configuration and the result of querying the surrogate for that configuration.","title":"Introduction and Installation"},{"location":"#introduction-and-installation","text":"JAHS-Bench-201 is the first collection of surrogate benchmarks for Joint Architecture and Hyperparameter Search (JAHS), built to also support and facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To install using pip run pip install git+https://github.com/automl/jahs_bench_201.git Optionally, you can download the data required to use the surrogate benchmark ahead of time with python -m jahs_bench.download --target surrogates To test if the installation was successful, you can, e.g, run a minimal example with python -m jahs_bench_examples.minimal This should randomly sample a configuration, and display both the sampled configuration and the result of querying the surrogate for that configuration.","title":"Introduction and Installation"},{"location":"authorstatement/","text":"The authors bear all responsibility in case of violation of rights.","title":"Author Statement"},{"location":"evaluation_protocol/","text":"For scientific studies we recommend an evaluation protocol when using JAHS-Bench-201 here to facilitate fair, reproducible, and methodologically sound comparisons. Optimization Tasks Studies with JAHS-Bench-201 should report on all three datasets and present results as trajectories of best validation error (single-objective) or hypervolume (multi-objective). Seeds and Errors Bounds Results should report the mean and standard error around the mean and feature a minimum of 10 seeds. Reporting Across Runtimes To allow using the same evaluation protocol and comparisons across various algorithmic settings the objective trajectories should be reported across total runtime taking into account the training and evaluation costs predicted by the surrogate benchmark. We suggest to report until a runtime corresponding to approximately 100 evaluations and, for interpretability, show the total runtime divided by the mean runtime of one evaluation (see below for an example). (Multi) Multi-fidelity For (multi) multi-fidelity runs utilizing epochs, we support both using continuations from few to many epochs (to simulate the checkpointing and continuation of the model) as well as retraining from scratch for optimizers that do not support continuations.","title":"Evaluation Protocol"},{"location":"evaluation_protocol/#optimization-tasks","text":"Studies with JAHS-Bench-201 should report on all three datasets and present results as trajectories of best validation error (single-objective) or hypervolume (multi-objective).","title":"Optimization Tasks"},{"location":"evaluation_protocol/#seeds-and-errors-bounds","text":"Results should report the mean and standard error around the mean and feature a minimum of 10 seeds.","title":"Seeds and Errors Bounds"},{"location":"evaluation_protocol/#reporting-across-runtimes","text":"To allow using the same evaluation protocol and comparisons across various algorithmic settings the objective trajectories should be reported across total runtime taking into account the training and evaluation costs predicted by the surrogate benchmark. We suggest to report until a runtime corresponding to approximately 100 evaluations and, for interpretability, show the total runtime divided by the mean runtime of one evaluation (see below for an example).","title":"Reporting Across Runtimes"},{"location":"evaluation_protocol/#multi-multi-fidelity","text":"For (multi) multi-fidelity runs utilizing epochs, we support both using continuations from few to many epochs (to simulate the checkpointing and continuation of the model) as well as retraining from scratch for optimizers that do not support continuations.","title":"(Multi) Multi-fidelity"},{"location":"leaderboards/","text":"JAHS-Bench-201 Leaderboards The leaderboards as explained in our paper. To add an entry please create a pull request changing docs/leaderboards.md . Make sure to follow our evaluation protocol and to add a URL to material that explains how to reproduce you results. Single Objective Black-box CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201 Cost-aware CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201 Multi-fidelity CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.78 \u00b1 0.14 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 95.27 \u00b1 0.04 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.13 \u00b1 0.03 Hyperband JAHS-Bench-201 Multi multi-fidelity CIFAR-10 Rank Accuracy \u00b1 SE Name URL 1 90.9 \u00b1 0.14 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Accuracy \u00b1 SE Name URL 1 94.91 \u00b1 0.07 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Accuracy \u00b1 SE Name URL 1 95.18 \u00b1 0.02 Hyperband JAHS-Bench-201 Multi Objective Black-box CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201 Cost-aware CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201 Multi-fidelity CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 1080.26 \u00b1 1.36 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 347.52 \u00b1 1.96 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 277.52 \u00b1 1.07 Hyperband JAHS-Bench-201 Multi multi-fidelity CIFAR-10 Rank Hypervolume \u00b1 SE Name URL 1 899.92 \u00b1 1.54 Hyperband JAHS-Bench-201 Colorectal-Histology Rank Hypervolume \u00b1 SE Name URL 1 330.76 \u00b1 0.22 Hyperband JAHS-Bench-201 Fashion-MNIST Rank Hypervolume \u00b1 SE Name URL 1 306.36 \u00b1 0.19 Hyperband JAHS-Bench-201","title":"Leaderboards"},{"location":"leaderboards/#jahs-bench-201-leaderboards","text":"The leaderboards as explained in our paper. To add an entry please create a pull request changing docs/leaderboards.md . Make sure to follow our evaluation protocol and to add a URL to material that explains how to reproduce you results.","title":"JAHS-Bench-201 Leaderboards"},{"location":"leaderboards/#single-objective","text":"","title":"Single Objective"},{"location":"leaderboards/#black-box","text":"","title":"Black-box"},{"location":"leaderboards/#cifar-10","text":"Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology","text":"Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist","text":"Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#cost-aware","text":"","title":"Cost-aware"},{"location":"leaderboards/#cifar-10_1","text":"Rank Accuracy \u00b1 SE Name URL 1 90.68 \u00b1 0.16 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_1","text":"Rank Accuracy \u00b1 SE Name URL 1 94.76 \u00b1 0.08 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_1","text":"Rank Accuracy \u00b1 SE Name URL 1 95.09 \u00b1 0.05 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-fidelity","text":"","title":"Multi-fidelity"},{"location":"leaderboards/#cifar-10_2","text":"Rank Accuracy \u00b1 SE Name URL 1 90.78 \u00b1 0.14 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_2","text":"Rank Accuracy \u00b1 SE Name URL 1 95.27 \u00b1 0.04 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_2","text":"Rank Accuracy \u00b1 SE Name URL 1 95.13 \u00b1 0.03 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-multi-fidelity","text":"","title":"Multi multi-fidelity"},{"location":"leaderboards/#cifar-10_3","text":"Rank Accuracy \u00b1 SE Name URL 1 90.9 \u00b1 0.14 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_3","text":"Rank Accuracy \u00b1 SE Name URL 1 94.91 \u00b1 0.07 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_3","text":"Rank Accuracy \u00b1 SE Name URL 1 95.18 \u00b1 0.02 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-objective","text":"","title":"Multi Objective"},{"location":"leaderboards/#black-box_1","text":"","title":"Black-box"},{"location":"leaderboards/#cifar-10_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_4","text":"Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#cost-aware_1","text":"","title":"Cost-aware"},{"location":"leaderboards/#cifar-10_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 1171.15 \u00b1 2.27 random search JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 278.6 \u00b1 2.23 random search JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_5","text":"Rank Hypervolume \u00b1 SE Name URL 1 231.88 \u00b1 3.86 random search JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-fidelity_1","text":"","title":"Multi-fidelity"},{"location":"leaderboards/#cifar-10_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 1080.26 \u00b1 1.36 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 347.52 \u00b1 1.96 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_6","text":"Rank Hypervolume \u00b1 SE Name URL 1 277.52 \u00b1 1.07 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"leaderboards/#multi-multi-fidelity_1","text":"","title":"Multi multi-fidelity"},{"location":"leaderboards/#cifar-10_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 899.92 \u00b1 1.54 Hyperband JAHS-Bench-201","title":"CIFAR-10"},{"location":"leaderboards/#colorectal-histology_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 330.76 \u00b1 0.22 Hyperband JAHS-Bench-201","title":"Colorectal-Histology"},{"location":"leaderboards/#fashion-mnist_7","text":"Rank Hypervolume \u00b1 SE Name URL 1 306.36 \u00b1 0.19 Hyperband JAHS-Bench-201","title":"Fashion-MNIST"},{"location":"performance_dataset/","text":"Details on Performance Dataset The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API, by running the following bash script: python -m jahs_bench.download --target = metric_data --save_dir = $save_dir where save_dir is a directory where the data will be stored. Nevertheless, interested users may directly download our DataFrames using a file transfer software of their choice, such as wget , from our archive. To download the full set of all metric datasets, run wget --no-parent -r https://ml.informatik.uni-freiburg.de/research-artifacts/jahs_bench_201/v1.0.0/metric_data.tar -O metric_data.tar tar -xf metric_data.tar Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task is the immediate sub-directory within \"metric_data\" and contains all the data pertaining to that task. Each task's directory contains the following 4 files: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"metric_data/cifar10/test_set.pkl.gz\" # Path to a downloaded file, ending in \".pkl.gz\" df = pd . read_pickle ( pth ) df . head ( 5 ) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Details on Performance Dataset"},{"location":"performance_dataset/#details-on-performance-dataset","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. Currently, we share all our data in the form of Pandas DataFrames which are very efficient for handling large tables of data, stored as compressed pickle files using pickle protocol 4. We are aware of the inherent limitations with sharing pickle files and therefore are investigating the most appropriate data format. Current candidates include CSV, HDF5 and Feather. The most convenient method for downloading our datasets is through our API, by running the following bash script: python -m jahs_bench.download --target = metric_data --save_dir = $save_dir where save_dir is a directory where the data will be stored. Nevertheless, interested users may directly download our DataFrames using a file transfer software of their choice, such as wget , from our archive. To download the full set of all metric datasets, run wget --no-parent -r https://ml.informatik.uni-freiburg.de/research-artifacts/jahs_bench_201/v1.0.0/metric_data.tar -O metric_data.tar tar -xf metric_data.tar","title":"Details on Performance Dataset"},{"location":"performance_dataset/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task is the immediate sub-directory within \"metric_data\" and contains all the data pertaining to that task. Each task's directory contains the following 4 files: * \"raw.pkl.gz\": This contains the full set of raw performance metrics sans any post-processing or filteration. * \"train_set.pkl.gz\": This is the actual training data used for training our surrogate models. * \"valid_set.pkl.gz\": This is the actual validation data used for validating the fitness of any given configuration during HPO. * \"test_set.pkl.gz\": This is the actual testing data used for generating the final performance scores of our surrogates reported in the paper. For each of these files, users can directly load them into memory as pandas DataFrames, as: import pandas as pd pth = \"metric_data/cifar10/test_set.pkl.gz\" # Path to a downloaded file, ending in \".pkl.gz\" df = pd . read_pickle ( pth ) df . head ( 5 ) The above code snippet, when filled in with a local path to the downloaded tarball, will display the first five rows in that table.","title":"Archive Structure"},{"location":"search_space/","text":"Design Space The design space of JAHS-Bench-201 has two components: the search space and the fidelity space. The search space comprises the joint architecture and hyperparameter space of the JAHS problem, whereas the fidelity space comprises parameters that directly alter the training cost with some trade-off in performance. We include these fidelity parameters in our design space to support benchmarking of multi-fidelity algorithms. For an overview of our design space see this table: Architecture Space We use the cell-based architecture search space from the popular and widely used tabular benchmark NAS-Bench-201 , which uses a directed acyclic graph with 4 vertices and 6 edges connecting them, for each of which there is a choice between an operation from the set of { skip-connect, zero, 1x1-conv, 3x3-conv, 3x3-avg-pool }. This accounts for a total of 5^6= 15.625 possible architectures. For an illustration adapted from the authors of NAS-Bench-201 see below. Hyperparameter Space Our hyperparameter space consists of a mix of categorical and continuous hyperparameters. As categorical hyperparameters, we added a choice between 3 activation functions (Mish, Hardswish or ReLU) in the convolution layers, as well as the choice to enable or disable the use of Trivial Augment for data augmentation in the training pipeline. For continuous hyperparametwe, we added the choice of learning rate and weight decay. Fidelity Space To support benchmarking of multi-fidelity algorithms, we consider four fidelity parameters that alter the training cost and performance: two architectural parameters controlling the network's modelling capacity, one to control the size of the input, and finally, the number of training epochs. Each cell may be repeated N in {1, 3, 5} times within a single block of the neural architecture. We call N the depth-multiplier. The first convolution layer of any cell contains W in {4, 8, 16} filters, with the number of filters doubling in every subsequent convolution layer in the same cell. Thus, we call W the width-multiplier. Additionally, we scaled the input images to different resolutions beforehand. This is denoted by the resolution-multiplier R in {0.25, 0.5, 1.0}. We can query the performance of every configuration at any epoch up to epoch 200. Therefore, we can use the number of epochs as a fidelity parameter, epoch in {1, 2, ..., 200}. In Code Configurations in our Joint Architecture and Hyperparameter (JAHS) space are represented as dictionaries, e.g.,: config = { 'Optimizer' : 'SGD' , 'LearningRate' : 0.1 , 'WeightDecay' : 5e-05 , 'Activation' : 'Mish' , 'TrivialAugment' : False , 'Op1' : 4 , 'Op2' : 1 , 'Op3' : 2 , 'Op4' : 0 , 'Op5' : 2 , 'Op6' : 1 , 'N' : 5 , 'W' : 16 , 'Resolution' : 1.0 , }","title":"Design Space"},{"location":"search_space/#design-space","text":"The design space of JAHS-Bench-201 has two components: the search space and the fidelity space. The search space comprises the joint architecture and hyperparameter space of the JAHS problem, whereas the fidelity space comprises parameters that directly alter the training cost with some trade-off in performance. We include these fidelity parameters in our design space to support benchmarking of multi-fidelity algorithms. For an overview of our design space see this table:","title":"Design Space"},{"location":"search_space/#architecture-space","text":"We use the cell-based architecture search space from the popular and widely used tabular benchmark NAS-Bench-201 , which uses a directed acyclic graph with 4 vertices and 6 edges connecting them, for each of which there is a choice between an operation from the set of { skip-connect, zero, 1x1-conv, 3x3-conv, 3x3-avg-pool }. This accounts for a total of 5^6= 15.625 possible architectures. For an illustration adapted from the authors of NAS-Bench-201 see below.","title":"Architecture Space"},{"location":"search_space/#hyperparameter-space","text":"Our hyperparameter space consists of a mix of categorical and continuous hyperparameters. As categorical hyperparameters, we added a choice between 3 activation functions (Mish, Hardswish or ReLU) in the convolution layers, as well as the choice to enable or disable the use of Trivial Augment for data augmentation in the training pipeline. For continuous hyperparametwe, we added the choice of learning rate and weight decay.","title":"Hyperparameter Space"},{"location":"search_space/#fidelity-space","text":"To support benchmarking of multi-fidelity algorithms, we consider four fidelity parameters that alter the training cost and performance: two architectural parameters controlling the network's modelling capacity, one to control the size of the input, and finally, the number of training epochs. Each cell may be repeated N in {1, 3, 5} times within a single block of the neural architecture. We call N the depth-multiplier. The first convolution layer of any cell contains W in {4, 8, 16} filters, with the number of filters doubling in every subsequent convolution layer in the same cell. Thus, we call W the width-multiplier. Additionally, we scaled the input images to different resolutions beforehand. This is denoted by the resolution-multiplier R in {0.25, 0.5, 1.0}. We can query the performance of every configuration at any epoch up to epoch 200. Therefore, we can use the number of epochs as a fidelity parameter, epoch in {1, 2, ..., 200}.","title":"Fidelity Space"},{"location":"search_space/#in-code","text":"Configurations in our Joint Architecture and Hyperparameter (JAHS) space are represented as dictionaries, e.g.,: config = { 'Optimizer' : 'SGD' , 'LearningRate' : 0.1 , 'WeightDecay' : 5e-05 , 'Activation' : 'Mish' , 'TrivialAugment' : False , 'Op1' : 4 , 'Op2' : 1 , 'Op3' : 2 , 'Op4' : 0 , 'Op5' : 2 , 'Op6' : 1 , 'N' : 5 , 'W' : 16 , 'Resolution' : 1.0 , }","title":"In Code"},{"location":"surrogate/","text":"Training a Surrogate Model Note: In order to use our surrogate training scripts, users must install the package Neural Pipeline Search (NePS) as an extra dependency. This can be done easily by running the following shell command in an environment already containing all the dependencies for JAHS-Bench-201: pip install neural-pipeline-search In case of any issues, users should consult the Git repo and documentation for NePS, which can be found here . Downloading the Data The data splits for training a model can be downloaded by running the following bash script: python -m jahs_bench.download --target = metric_data --save_dir = $save_dir where save_dir is a directory where the data will be stored. Training the Models We train one surrogate model per metric. The downloaded data files are already arranged in a directory structure with the format <save_dir>/metric_data/<task>/<metric>/{train,valid,test}_set.pkl.gz . Thus, for training a surrogate for predicting, for instance, the validation accuracy scores on CIFAR-10 over 20 HPO evaluations, one would then run the following shell script: python -m jahs_bench.surrogate_training.pipeline --working_directory = $root_dir / $metric --datadir = $save_dir /metric_data/ $task --output = $metric --max_evaluations_total = 20 Here, root_dir is a directory where all the interim data files and logs generated during the HPO loop, including the trained interim surrogate models, will be stored for one given task. The above command can be executed by multiple parallel workers as long as all the input arguments remain fixed in order to achieve parallelization (i.e. when launched with 4 workers, all workers will stop once 20 total evaluations are reached). Assembling the Final Ensemble Once the above HPO loop finishes, it is necessary to extract the best model for each metric and put them together into an ensemble that can be used by the Benchmark API. In order to do this, run the following shell script: python -m jahs_bench.surrogate_training.assemble_models --final_dir = $final_dir --root_dir = $root_dir where final_dir is the directory where the ensemble for one given task should be saved and root_dir is the same as in the previous step. Evaluating the Trained Surrogates Finally, the trained surrogates can be evaluated on the test set to obtain the final correlation and regression scores. This can be achieved by running the following shell script: outputs =( latency runtime valid-acc ) python -m jahs_bench.surrogate_training.evaluation --testset-file = $path_to_test_set --model-dir = $final_dir --save-dir = $save_dir --outputs ${ outputs [@] } Where path_to_test_set is the full path to the relevant test_set.pkl.gz file of the task the surrogate was trained for, final_dir is the directory where a surrogate ensemble was saved in the previous step, save_dir is a directory where the results of running this script - including surrogate predictions on the test set and the generated scores - should be saved. Finally, depending on which metrics the surrogate was trained to predict, the array outputs should be set accordingly. Here, we show an example using the metrics \"latency\", \"runtime\" and \"valid-acc\" (validation accuracy). Cluster-based Computation We note that our own experiments were run on a distributed computing cluster, which entails cluster-specific details and configurations, but used the same scripts as described here at their heart. Depending on their own local setup, users may thus have to tweak their usage of the scripts appropriately. These scripts may also contain additional options to help users with this setup, accessible by running python -m $script_name.py --help . Downloading the Surrogate Models The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as compressed tarballs that are readable using our code base. The most convenient method for downloading our models is through our API . Nevertheless, interested users may directly download our DataFrames using a file transfer software of their choice, such as wget , from our archive. To download the full set of all surrogates models, run wget --no-parent -r https://ml.informatik.uni-freiburg.de/research-artifacts/jahs_bench_201/v1.0.0/assembled_surrogates.tar -O assembled_surrogates.tar tar -xf assembled_surrogates.tar Archive Structure For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task is the immediate sub-directory within \"metric_data\" and contains all the models pertaining to that task. Immediately under each task's directory, are further sub-directories named after the individual metrics the models they contains were trained to predict. These sub-directories can be directly passed to jahs_bench.surrogate.model.XGBSurrogate.load() in order to load the respective models into memory. The downloaded models can be individually loaded into memory as: from jahs_bench.surrogate.model import XGBSurrogate pth = \"assembled_surrogates/cifar10/latency\" # Path to a model directory model = XGBSurrogate . load ( pth ) The advantage to directly loading a model in this manner is that the model.predict() method is able to process entire DataFrames of queries and return a corresponding DataFrame of predicted metrics.","title":"Details on Surrogate Models"},{"location":"surrogate/#training-a-surrogate-model","text":"Note: In order to use our surrogate training scripts, users must install the package Neural Pipeline Search (NePS) as an extra dependency. This can be done easily by running the following shell command in an environment already containing all the dependencies for JAHS-Bench-201: pip install neural-pipeline-search In case of any issues, users should consult the Git repo and documentation for NePS, which can be found here .","title":"Training a Surrogate Model"},{"location":"surrogate/#downloading-the-data","text":"The data splits for training a model can be downloaded by running the following bash script: python -m jahs_bench.download --target = metric_data --save_dir = $save_dir where save_dir is a directory where the data will be stored.","title":"Downloading the Data"},{"location":"surrogate/#training-the-models","text":"We train one surrogate model per metric. The downloaded data files are already arranged in a directory structure with the format <save_dir>/metric_data/<task>/<metric>/{train,valid,test}_set.pkl.gz . Thus, for training a surrogate for predicting, for instance, the validation accuracy scores on CIFAR-10 over 20 HPO evaluations, one would then run the following shell script: python -m jahs_bench.surrogate_training.pipeline --working_directory = $root_dir / $metric --datadir = $save_dir /metric_data/ $task --output = $metric --max_evaluations_total = 20 Here, root_dir is a directory where all the interim data files and logs generated during the HPO loop, including the trained interim surrogate models, will be stored for one given task. The above command can be executed by multiple parallel workers as long as all the input arguments remain fixed in order to achieve parallelization (i.e. when launched with 4 workers, all workers will stop once 20 total evaluations are reached).","title":"Training the Models"},{"location":"surrogate/#assembling-the-final-ensemble","text":"Once the above HPO loop finishes, it is necessary to extract the best model for each metric and put them together into an ensemble that can be used by the Benchmark API. In order to do this, run the following shell script: python -m jahs_bench.surrogate_training.assemble_models --final_dir = $final_dir --root_dir = $root_dir where final_dir is the directory where the ensemble for one given task should be saved and root_dir is the same as in the previous step.","title":"Assembling the Final Ensemble"},{"location":"surrogate/#evaluating-the-trained-surrogates","text":"Finally, the trained surrogates can be evaluated on the test set to obtain the final correlation and regression scores. This can be achieved by running the following shell script: outputs =( latency runtime valid-acc ) python -m jahs_bench.surrogate_training.evaluation --testset-file = $path_to_test_set --model-dir = $final_dir --save-dir = $save_dir --outputs ${ outputs [@] } Where path_to_test_set is the full path to the relevant test_set.pkl.gz file of the task the surrogate was trained for, final_dir is the directory where a surrogate ensemble was saved in the previous step, save_dir is a directory where the results of running this script - including surrogate predictions on the test set and the generated scores - should be saved. Finally, depending on which metrics the surrogate was trained to predict, the array outputs should be set accordingly. Here, we show an example using the metrics \"latency\", \"runtime\" and \"valid-acc\" (validation accuracy).","title":"Evaluating the Trained Surrogates"},{"location":"surrogate/#cluster-based-computation","text":"We note that our own experiments were run on a distributed computing cluster, which entails cluster-specific details and configurations, but used the same scripts as described here at their heart. Depending on their own local setup, users may thus have to tweak their usage of the scripts appropriately. These scripts may also contain additional options to help users with this setup, accessible by running python -m $script_name.py --help .","title":"Cluster-based Computation"},{"location":"surrogate/#downloading-the-surrogate-models","text":"The current hosting solution is a transitory one as we work towards setting up a more robust solution using Figshare+ , which provides perpetual data storage guarantees, a DOI and a web API for querying the dataset as well as the metadata. We share our trained models as compressed tarballs that are readable using our code base. The most convenient method for downloading our models is through our API . Nevertheless, interested users may directly download our DataFrames using a file transfer software of their choice, such as wget , from our archive. To download the full set of all surrogates models, run wget --no-parent -r https://ml.informatik.uni-freiburg.de/research-artifacts/jahs_bench_201/v1.0.0/assembled_surrogates.tar -O assembled_surrogates.tar tar -xf assembled_surrogates.tar","title":"Downloading the Surrogate Models"},{"location":"surrogate/#archive-structure","text":"For each of the three tasks, \"cifar10\", \"colorectal_histology\" and \"fashion_mnist\", the name of the task is the immediate sub-directory within \"metric_data\" and contains all the models pertaining to that task. Immediately under each task's directory, are further sub-directories named after the individual metrics the models they contains were trained to predict. These sub-directories can be directly passed to jahs_bench.surrogate.model.XGBSurrogate.load() in order to load the respective models into memory. The downloaded models can be individually loaded into memory as: from jahs_bench.surrogate.model import XGBSurrogate pth = \"assembled_surrogates/cifar10/latency\" # Path to a model directory model = XGBSurrogate . load ( pth ) The advantage to directly loading a model in this manner is that the model.predict() method is able to process entire DataFrames of queries and return a corresponding DataFrame of predicted metrics.","title":"Archive Structure"},{"location":"usage/","text":"Querying JAHS-Bench-201 Evaluating Configurations with the Surrogate import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , download = True ) # Query a random configuration config = benchmark . sample_config () results = benchmark ( config , nepochs = 200 ) # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict of dicts, indexed first by epoch and then by metric name Querying the Full Trajectories Optionally, the full trajectory of a query can be queried by flipping a single flag config = benchmark . sample_config () trajectory = benchmark ( config , nepochs = 200 , full_trajectory = True ) print ( trajectory ) # A dict of dicts More Evaluation Options The API of our benchmark enables users to either query a surrogate model (the default) or the tables of performance data, or train a configuration from our search space from scratch using the same pipeline as was used by our benchmark. However, users should note that the latter functionality requires the installation of jahs_bench_201 with the optional data_creation component and its relevant dependencies. The relevant data can be automatically downloaded by our API. Users may switch between querying the surrogate model, the performance dataset, or a live training of a configuration by passing one of the strings surrogate (default), table or live to the parameter kind when initializing the Benchmark object, as: benchmark_surrogate = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"surrogate\" , download = True ) benchmark_tabular = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) benchmark_live = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"live\" , download = True ) Setting the flag download to True allows the API to automatically fetch all the relevant data files over the internet. This includes the surrogate models, the performance dataset DataFrame objects, and the task datasets and their splits, depending on whether kind was set to surrogate , table or live . Note: Before using the option kind=\"live\" , it is necessary to ensure that JAHS-Bench-201 has been installed with the extra dependencies required for live training. This can be done using the following command: pip install \"jahs_bench[data_creation] @ git+https://github.com/automl/jahs_bench_201.git\" More Examples Querying the Performance Tables # Download the performance dataset import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) # Query a random configuration config = benchmark . sample_config () results = benchmark ( config , nepochs = 200 ) # Display the outputs print ( f \"Config: { config } \" ) print ( f \"Result: { results } \" ) Querying the Performance Tables with Full Trajectory # Download the performance dataset import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) # Query a random configuration config = benchmark . sample_config () trajectory = benchmark ( config , nepochs = 200 , full_trajectory = True ) # Display the outputs print ( f \"Config: { config } \" ) print ( f \"Result: { trajectory } \" ) Live Training a Random Configuration from Scratch with Full Trajectory # Initialize the pipeline import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"live\" , download = True ) # Query a random configuration config = benchmark . sample_config () trajectory = benchmark ( config , nepochs = 200 , full_trajectory = True ) # Display the outputs print ( f \"Config: { config } \" ) print ( f \"Result: { trajectory } \" )","title":"Querying the Benchmark"},{"location":"usage/#querying-jahs-bench-201","text":"","title":"Querying JAHS-Bench-201"},{"location":"usage/#evaluating-configurations-with-the-surrogate","text":"import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , download = True ) # Query a random configuration config = benchmark . sample_config () results = benchmark ( config , nepochs = 200 ) # Display the outputs print ( f \"Config: { config } \" ) # A dict print ( f \"Result: { results } \" ) # A dict of dicts, indexed first by epoch and then by metric name","title":"Evaluating Configurations with the Surrogate"},{"location":"usage/#querying-the-full-trajectories","text":"Optionally, the full trajectory of a query can be queried by flipping a single flag config = benchmark . sample_config () trajectory = benchmark ( config , nepochs = 200 , full_trajectory = True ) print ( trajectory ) # A dict of dicts","title":"Querying the Full Trajectories"},{"location":"usage/#more-evaluation-options","text":"The API of our benchmark enables users to either query a surrogate model (the default) or the tables of performance data, or train a configuration from our search space from scratch using the same pipeline as was used by our benchmark. However, users should note that the latter functionality requires the installation of jahs_bench_201 with the optional data_creation component and its relevant dependencies. The relevant data can be automatically downloaded by our API. Users may switch between querying the surrogate model, the performance dataset, or a live training of a configuration by passing one of the strings surrogate (default), table or live to the parameter kind when initializing the Benchmark object, as: benchmark_surrogate = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"surrogate\" , download = True ) benchmark_tabular = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) benchmark_live = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"live\" , download = True ) Setting the flag download to True allows the API to automatically fetch all the relevant data files over the internet. This includes the surrogate models, the performance dataset DataFrame objects, and the task datasets and their splits, depending on whether kind was set to surrogate , table or live . Note: Before using the option kind=\"live\" , it is necessary to ensure that JAHS-Bench-201 has been installed with the extra dependencies required for live training. This can be done using the following command: pip install \"jahs_bench[data_creation] @ git+https://github.com/automl/jahs_bench_201.git\"","title":"More Evaluation Options"},{"location":"usage/#more-examples","text":"","title":"More Examples"},{"location":"usage/#querying-the-performance-tables","text":"# Download the performance dataset import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) # Query a random configuration config = benchmark . sample_config () results = benchmark ( config , nepochs = 200 ) # Display the outputs print ( f \"Config: { config } \" ) print ( f \"Result: { results } \" )","title":"Querying the Performance Tables"},{"location":"usage/#querying-the-performance-tables-with-full-trajectory","text":"# Download the performance dataset import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"table\" , download = True ) # Query a random configuration config = benchmark . sample_config () trajectory = benchmark ( config , nepochs = 200 , full_trajectory = True ) # Display the outputs print ( f \"Config: { config } \" ) print ( f \"Result: { trajectory } \" )","title":"Querying the Performance Tables with Full Trajectory"},{"location":"usage/#live-training-a-random-configuration-from-scratch-with-full-trajectory","text":"# Initialize the pipeline import jahs_bench benchmark = jahs_bench . Benchmark ( task = \"cifar10\" , kind = \"live\" , download = True ) # Query a random configuration config = benchmark . sample_config () trajectory = benchmark ( config , nepochs = 200 , full_trajectory = True ) # Display the outputs print ( f \"Config: { config } \" ) print ( f \"Result: { trajectory } \" )","title":"Live Training a Random Configuration from Scratch with Full Trajectory"}]}