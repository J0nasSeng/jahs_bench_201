"""
This script is intended to be used for pre-computing a more efficient schedule for distributing model configuration
evaluations across the workers in a job allocation using data that has been generated from a smaller run over all the
planned samples in the search space.
"""


import argparse
import itertools
import logging
import pandas as pd
from pathlib import Path
from string import Template
import sys

import naslib.utils.logging as naslib_logging
import naslib.utils.utils as naslib_utils
from naslib.utils.utils import AttrDict

import tabular_sampling.lib.postprocessing.metric_df_ops
from tabular_sampling.clusterlib import prescheduler as sched_utils
from tabular_sampling.lib.constants import Datasets, standard_task_metrics
from tabular_sampling.lib.constants import training_config as _training_config
from tabular_sampling.lib import utils
from tabular_sampling.lib import datasets as dataset_lib
from tabular_sampling.lib.procs import train
from tabular_sampling.lib.postprocessing.metric_df_ops import get_configs
from tabular_sampling.search_space import NASB201HPOSearchSpace

_log = logging.getLogger(__name__)


def argument_parser():
    parser = argparse.ArgumentParser()

    parser.add_argument("--metrics_df", type=Path,
                        help="Path to a pandas DataFrame that contains all the metric data that has been collected "
                             "thus far and is to be used for generating the schedule.")
    parser.add_argument("--cpus_per_worker", type=int,
                        help="Job configuration - the number of CPUs to be allocated to each worker.")
    parser.add_argument("--cpus_per_node", type=int, default=48,
                        help="Job configuration - the number of CPUs that are to be expected in each node of the "
                             "cluster.")
    parser.add_argument("--nodes_per_job", type=int,
                        help="The number of nodes that are available per independent job.")
    parser.add_argument("--timelimit", type=int, default=24 * 60,
                        help="The maximum amount of time (in minutes) that a single job is allowed to run for.")
    parser.add_argument("--dynamic_timelimit", action="store_true",
                        help="When this flag is given, the time limit of the job is dynamically adjusted to maximize "
                             "the CPUh utilization. The dynamically adjusted time limit can only be lower than that "
                             "specified by '--timelimit'. If this flag is omitted, the job will have exactly the value "
                             "of '--timelimit' as its time limit.")
    parser.add_argument("--epochs", type=int,
                        help="The maximum number of epochs that each configuration should be evaluated for.")
    parser.add_argument("--cpuh_utilization_cutoff", type=float, default=0.8,
                        help="Recommended minimum fraction of total allocated CPUh that should be actively used for "
                             "computation. Generates a warning when the job allocation's expected CPUh utilization is "
                             "below this value.")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode (very verbose) logging.")
    parser.add_argument("--template_dir", type=Path,
                        help="Path to a directory which contains templates using which the job files and their "
                             "relevant srun config files are generated. The templates for jobs and configs should be "
                             "named 'job.template' and 'config.template' respectively.")
    parser.add_argument("--portfolio_dir", type=Path,
                        help="Path to a directory from where each worker will be able to store its own allocated "
                             "portfolio of configurations to evaluate.")
    parser.add_argument("--script_dir", type=Path, help="This is the directory where the generated job scripts will be "
                                                        "stored.")
    parser.add_argument("--slurm_dir", type=Path,
                        help="This is the directory where the outputs of the jobs will be stored i.e. WORK. The base "
                             "directory for each DirectoryTree will be '<slurm_dir>/<fidelity_dir>/tasks', where "
                             "'fidelity_dir' is generated by joining the names and values of the relevant fidelity "
                             "parameters in a string separated by '-', e.g. 'N-1-W-8-Resolution-1.0'.")

    return parser


if __name__ == "__main__":

    # Setup this module's logger
    fmt = logging.Formatter("[%(asctime)s] %(name)s %(levelname)s: %(message)s", datefmt="%m/%d %H:%M:%S")
    ch = logging.StreamHandler(stream=sys.stdout)
    ch.setLevel(logging.DEBUG)
    ch.setFormatter(fmt)
    _log.addHandler(ch)
    _log.setLevel(logging.INFO)

    sched_utils._log.addHandler(ch)
    sched_utils._log.setLevel(logging.INFO)

    ## Parse CLI
    args = argument_parser().parse_args()

    if args.debug:
        _log.setLevel(logging.DEBUG)
        sched_utils._log.setLevel(logging.DEBUG)

    metrics_df = pd.read_pickle(args.metrics_df)
    configs = get_configs(df=metrics_df)
    estimated_runtimes = tabular_sampling.lib.postprocessing.metric_df_ops.estimate_remaining_runtime(metrics_df, max_epochs=args.epochs)
    estimated_runtimes = pd.concat({"model_config": configs, "runtime": estimated_runtimes}, axis=1)
    _log.info(f"Estimated total CPUh requirement: "
              f"{estimated_runtimes[('runtime', 'required')].sum() * args.cpus_per_worker:,}")

    job_config = sched_utils.JobConfig(
        cpus_per_worker=args.cpus_per_worker, cpus_per_node=args.cpus_per_node, nodes_per_job=args.nodes_per_job,
        timelimit=args.timelimit * 60
    )
    workers = sched_utils.allocate_work(
        job_config=job_config, runtime_estimates=estimated_runtimes,
        cpuh_utilization_cutoff=args.cpuh_utilization_cutoff, cap_job_timelimit=args.dynamic_timelimit
    )

    sched_utils.save_worker_portfolios(workers=workers, portfolio_dir=args.portfolio_dir)

    with open(args.template_dir / "job.template") as fp:
        job_template = Template(fp.read())

    with open(args.template_dir / "config.template") as fp:
        config_template = Template(fp.read())

    ctr = itertools.count(start=1)
    workerid_offset = 0

    for _ in workers[::job_config.workers_per_job]:
        jobid = next(ctr)
        job_name = f"resume_{jobid}"
        jobdir = str(args.slurm_dir)
        job_str = job_template.substitute(
            jobdir=jobdir, scriptdir=args.script_dir, job_name=job_name, **job_config.template_kwargs
        )
        srun_str = config_template.substitute(
            rootdir=jobdir, workerid_offset=workerid_offset, portfolio_dir=str(args.portfolio_dir),
            epochs=str(args.epochs)
        )

        with open(args.script_dir / f"job-{jobid}.job", "w") as fp:
            fp.write(job_str)

        with open(args.script_dir / f"job-{jobid}.config", "w") as fp:
            fp.write(srun_str)

        workerid_offset += job_config.workers_per_job
